# ===== On-Prem AI Note Taker - VPS Environment Configuration =====
# Copy this file to .env on your VPS and customize as needed

# =============================================================================
# VPS SERVER CONFIGURATION (This file goes on your VPS)
# =============================================================================

# ===== Application Server =====
APP_HOST=0.0.0.0                    # Bind to all interfaces (required for external access)
APP_PORT=8000                       # Backend API port

# ===== Whisper AI Configuration (OPTIMIZED for VPS) =====
WHISPER_MODEL=tiny                  # Model size: tiny=3x faster, base=balanced, small/medium/large=slower
WHISPER_COMPUTE_TYPE=int8           # Compute type: int8=fastest, int16=slower, float16/32=slowest
WHISPER_DEVICE=cpu                  # Force CPU usage for VPS
WHISPER_CPU_THREADS=6               # Optimize for 6 vCPU VPS (use 6 out of 8 cores)
WHISPER_MEMORY_LIMIT_GB=16          # Memory limit for Whisper (leaves 8GB for system)
WHISPER_BEAM_SIZE=1                 # Beam size for CPU optimization (1=fastest, 5=better quality)
WHISPER_DOWNLOAD_ROOT=./models      # Local model storage

# ===== Ollama Configuration (OPTIMIZED for VPS) =====
OLLAMA_BASE_URL=http://ollama:11434  # Ollama service URL (Docker service name for internal communication)
OLLAMA_MODEL=qwen2.5:3b-instruct    # OPTIMIZED: Fast 3B model perfect for meeting notes (1.9GB, 6-7s response)
OLLAMA_TIMEOUT_SECONDS=3600         # Request timeout (1 hour for slow VPS/models)
OLLAMA_CPU_THREADS=12               # OPTIMIZED: Double CPU threads for better performance
OLLAMA_MEMORY_LIMIT_GB=18           # OPTIMIZED: Increased memory for better model performance

# ===== Ollama Performance Optimizations =====
OLLAMA_NUM_PARALLEL=6               # OPTIMIZED: Use all 6 vCPU cores for parallel processing
OLLAMA_CPU_AVX=1                    # Enable AVX instructions for better CPU performance
OLLAMA_CPU_AVX2=1                   # Enable AVX2 instructions for better CPU performance
OLLAMA_CPU_F16=1                    # Enable F16 precision for better performance
OLLAMA_CPU_MKL=1                    # Enable Intel MKL optimization

# ===== Performance & Resources =====
MAX_CONCURRENCY=3                    # OPTIMIZED: Increased to 3 concurrent transcriptions (using all 6 vCPU cores)
MAX_UPLOAD_MB=200                    # OPTIMIZED: Increased to 200MB for longer meetings (supports 2+ hour meetings)

# ===== Language Restrictions (OPTIMIZED for speed) =====
ALLOWED_LANGUAGES=tr,en             # OPTIMIZED: EN+TR only (20-25% speed boost vs auto-detect)
FORCE_LANGUAGE_VALIDATION=true       # Strict language validation (prevents errors)

# ===== Speaker Identification (NEW FEATURE) =====
ENABLE_SPEAKER_IDENTIFICATION=true   # Enable speaker differentiation in transcripts
SPEAKER_CHANGE_THRESHOLD_MS=1000    # Speaker change detection threshold (1 second silence)
MAX_SPEAKERS=5                       # Maximum number of speakers to identify

# ===== Progress Tracking =====
ENABLE_PROGRESS_TRACKING=true        # Enable progress tracking
PROGRESS_UPDATE_INTERVAL_MS=500      # Progress update frequency
ENABLE_SSE=true                      # Enable Server-Sent Events

# ===== Queue System =====
USE_QUEUE_SYSTEM=true                # Enable Redis-based job queuing
REDIS_URL=redis://redis:6379         # Redis connection URL (Docker service name for internal communication)
QUEUE_MAX_WORKERS=2                  # Maximum concurrent workers

# ===== Authentication (REQUIRED for Security) =====
# Set these to secure your API - CHANGE THESE PASSWORDS!
BASIC_AUTH_USERNAME=myca
BASIC_AUTH_PASSWORD=wj2YyxrJ4cqcXgCA

# ===== Logging =====
LOG_LEVEL=INFO                       # Log level: DEBUG, INFO, WARNING, ERROR

# ===== CORS & Security =====
# Comma-separated list. Examples:
#   "*" (allow all) OR
#   "http://localhost:5173,http://95.111.244.159,null" (recommended)
ALLOWED_ORIGINS=http://localhost:5173,http://95.111.244.159,null

# =============================================================================
# CLIENT CONFIGURATION (Copy to your laptop's .env.local file)
# =============================================================================
# Your frontend on your laptop should have this in .env.local:
# VITE_API_BASE_URL=http://95.111.244.159:8000/api

# =============================================================================
# LOCAL DEV TEMPLATES (Copy-paste ready)
# =============================================================================
# backend/.env.local (run on your laptop when developing backend locally)
# -------------------------------------------------------
# APP_HOST=0.0.0.0
# APP_PORT=8001
# LOG_LEVEL=INFO
# WHISPER_MODEL=tiny
# WHISPER_COMPUTE_TYPE=int8
# WHISPER_DEVICE=cpu
# WHISPER_CPU_THREADS=6
# WHISPER_MEMORY_LIMIT_GB=16
# WHISPER_BEAM_SIZE=1
# WHISPER_DOWNLOAD_ROOT=./models
# OLLAMA_BASE_URL=http://95.111.244.159:11434
# OLLAMA_MODEL=qwen2.5:3b-instruct
# OLLAMA_TIMEOUT_SECONDS=180
# OLLAMA_CPU_THREADS=12
# OLLAMA_MEMORY_LIMIT_GB=18
# ALLOWED_LANGUAGES=tr,en
# FORCE_LANGUAGE_VALIDATION=true
# MAX_CONCURRENCY=3
# MAX_UPLOAD_MB=200
# USE_QUEUE_SYSTEM=true
# REDIS_URL=redis://95.111.244.159:6385
# QUEUE_MAX_WORKERS=2
# BASIC_AUTH_USERNAME=myca
# BASIC_AUTH_PASSWORD=wj2YyxrJ4cqcXgCA
# ALLOWED_ORIGINS=http://localhost:5173,http://95.111.244.159,null
# ENABLE_PROGRESS_TRACKING=true
# PROGRESS_UPDATE_INTERVAL_MS=500
# ENABLE_SSE=true

# frontend/.env.local (run on your laptop for frontend)
# -------------------------------------------------------
# VITE_API_BASE_URL=http://95.111.244.159:8000/api
# VITE_BASIC_AUTH_USERNAME=myca
# VITE_BASIC_AUTH_PASSWORD=wj2YyxrJ4cqcXgCA
# VITE_DEV_MODE=true
# VITE_LOG_LEVEL=info
# VITE_ENABLE_PROGRESS_TRACKING=true
# VITE_ENABLE_SSE=true
# VITE_ENABLE_JOB_MANAGEMENT=true
# VITE_ENABLE_SPEAKER_IDENTIFICATION=true
# VITE_ENABLE_TURKISH_SUPPORT=true
# VITE_DEFAULT_LANGUAGE=auto
# VITE_SHOW_LANGUAGE_SELECTOR=true
# VITE_SHOW_PROGRESS_BARS=true
# VITE_SHOW_ETA=true
# VITE_PROGRESS_UPDATE_INTERVAL=500
# VITE_SSE_TIMEOUT=30000
# VITE_MAX_FILE_SIZE_MB=200
# VITE_APP_TITLE="AI Note Taker"
# VITE_APP_DESCRIPTION="AI-powered note taking with transcription and summarization"
# VITE_APP_VERSION="1.0.0"
# VITE_OLLAMA_MODEL=qwen2.5:3b-instruct
# VITE_WHISPER_MODEL=tiny
# VITE_SUPPORTED_LANGUAGES=tr,en,auto

# =============================================================================
# VPS INFORMATION (for reference) =====
VPS_HOST=95.111.244.159
VPS_USER=$USER
VPS_PORT=22

# =============================================================================
# IMPORTANT NOTES =====
# 1. This file goes on your VPS at /myca/on-prem-ai-note-taker/.env
# 2. Your laptop frontend needs .env.local with VITE_API_BASE_URL=http://95.111.244.159:8000/api
# 3. Backend runs on VPS at http://95.111.244.159:8000
# 4. Ollama runs on VPS at http://95.111.244.159:11434
# 5. Frontend runs on your laptop at http://localhost:5173
# 6. Set up firewall rules to restrict access to your VPS
# 7. Consider using VPN or SSH tunneling for additional security
# 8. NEW: Using qwen2.5:3b-instruct model for optimal performance (6-7s response time)
# 9. NEW: Speaker identification enabled for better meeting transcripts
# 10. NEW: Optimized for 6 vCPU VPS with 18GB memory allocation
