# version: "3.8"  # Obsolete in Docker Compose v2
services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    deploy:
      resources:
        limits:
          cpus: '6.0'  # Use all 6 vCPU cores for better performance
          memory: 18G  # Increased from 12G to 18G for better model performance
        reservations:
          cpus: '3.0'
          memory: 12G
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_NUM_PARALLEL=6  # Use all 6 vCPU cores
      - OLLAMA_CPU_THREADS=12  # Double the threads for better performance
      - OLLAMA_GPU_LAYERS=0    # CPU-only mode
      - OLLAMA_CPU_AVX=1       # Enable AVX instructions
      - OLLAMA_CPU_AVX2=1      # Enable AVX2 instructions
      - OLLAMA_CPU_F16=1       # Enable F16 precision
      - OLLAMA_CPU_MKL=1       # Enable Intel MKL optimization
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    restart: unless-stopped
    depends_on:
      - ollama
      - redis
    environment:
      APP_HOST: 0.0.0.0
      APP_PORT: 8000
      ALLOWED_ORIGINS: "*"
      WHISPER_MODEL: ${WHISPER_MODEL:-tiny}
      WHISPER_COMPUTE_TYPE: ${WHISPER_COMPUTE_TYPE:-int8}
      WHISPER_DEVICE: ${WHISPER_DEVICE:-cpu}
      WHISPER_CPU_THREADS: ${WHISPER_CPU_THREADS:-6}
      WHISPER_MEMORY_LIMIT_GB: ${WHISPER_MEMORY_LIMIT_GB:-16}
      WHISPER_DOWNLOAD_ROOT: ./models
      OLLAMA_BASE_URL: http://ollama:11434  # Fixed: Docker service name for internal communication
      OLLAMA_MODEL: ${OLLAMA_MODEL:-qwen2.5:3b-instruct}
      OLLAMA_TIMEOUT_SECONDS: ${OLLAMA_TIMEOUT_SECONDS:-3600}
      OLLAMA_CPU_THREADS: ${OLLAMA_CPU_THREADS:-12}
      OLLAMA_MEMORY_LIMIT_GB: ${OLLAMA_MEMORY_LIMIT_GB:-18}
      BASIC_AUTH_USERNAME: ${BASIC_AUTH_USERNAME:-myca}
      BASIC_AUTH_PASSWORD: ${BASIC_AUTH_PASSWORD:-wj2YyxrJ4cqcXgCA}
      REDIS_URL: redis://redis:6379  # Fixed: use internal Redis port
      QUEUE_MAX_WORKERS: ${QUEUE_MAX_WORKERS:-2}
      USE_QUEUE_SYSTEM: ${USE_QUEUE_SYSTEM:-true}
      MAX_CONCURRENCY: ${MAX_CONCURRENCY:-2}
      MAX_UPLOAD_MB: ${MAX_UPLOAD_MB:-100}
      ALLOWED_LANGUAGES: ${ALLOWED_LANGUAGES:-tr,en}
      FORCE_LANGUAGE_VALIDATION: ${FORCE_LANGUAGE_VALIDATION:-true}
      ENABLE_PROGRESS_TRACKING: ${ENABLE_PROGRESS_TRACKING:-true}
      PROGRESS_UPDATE_INTERVAL_MS: ${PROGRESS_UPDATE_INTERVAL_MS:-500}
      ENABLE_SSE: ${ENABLE_SSE:-true}
    ports:
      - "8000:8000"
    deploy:
      resources:
        limits:
          cpus: '4.0'  # Backend processing
          memory: 8G
        reservations:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - whisper_models:/models

  # Frontend will run on your local computer, not on VPS
  # frontend:
  #   build:
  #     context: ./frontend
  #     dockerfile: Dockerfile
  #   restart: unless-stopped
  #   depends_on:
  #     - backend
  #   ports:
  #     - "8080:80"
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget -q --spider http://localhost/ || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    ports:
      - "6385:6379"
    volumes:
      - redis_data:/data
    deploy:
      resources:
        limits:
          cpus: '1.0'  # Minimal Redis resources
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  ollama_models:
  whisper_models:
  redis_data:

