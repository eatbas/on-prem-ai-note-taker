@echo off
setlocal enabledelayedexpansion

echo üöÄ Llama 3.1 8B Performance Diagnostics
echo ================================================
echo VPS: 95.111.244.159
echo Backend Port: 8000
echo Ollama Port: 11434
echo.

REM Test basic connectivity
echo üì° Testing Connectivity...
echo ================================================

echo üîç Testing Backend Health...
curl -s -w "HTTP: %%{http_code}, Time: %%{time_total}s" --max-time 30 "http://95.111.244.159:8000/api/health" >nul 2>&1
if !errorlevel! equ 0 (
    echo ‚úÖ Backend Health: OK
) else (
    echo ‚ùå Backend Health: FAILED
)

echo.
echo üîç Testing Ollama Service...
curl -s -w "HTTP: %%{http_code}, Time: %%{time_total}s" --max-time 30 "http://95.111.244.159:11434/api/tags" >nul 2>&1
if !errorlevel! equ 0 (
    echo ‚úÖ Ollama Service: OK
) else (
    echo ‚ùå Ollama Service: FAILED
)

echo.
echo ü§ñ Testing Llama Chat Performance...
echo ================================================

REM Test 1: Simple question
echo üîç Test 1: Simple Math Question
echo Prompt: "What is 2+2?"
echo Max Tokens: 50
echo.

set start_time=%time%
curl -s -w "HTTP: %%{http_code}, Time: %%{time_total}s" --max-time 120 ^
    -H "Content-Type: application/json" ^
    -H "Authorization: Basic bXljYTp3ajJZeXhqNGNxY1hnQ0E=" ^
    -d "{\"prompt\": \"What is 2+2?\", \"model\": \"llama3.1:8b\", \"stream\": false, \"options\": {\"num_predict\": 50, \"temperature\": 0.7, \"top_p\": 0.9}}" ^
    "http://95.111.244.159:8000/api/chat"
set end_time=%time%

echo.
echo ‚è±Ô∏è  Response Time: Measured
echo.

REM Test 2: Medium complexity
echo üîç Test 2: Medium Complexity Question
echo Prompt: "Explain machine learning in simple terms"
echo Max Tokens: 100
echo.

curl -s -w "HTTP: %%{http_code}, Time: %%{time_total}s" --max-time 120 ^
    -H "Content-Type: application/json" ^
    -H "Authorization: Basic bXljYTp3ajJZeXhqNGNxY1hnQ0E=" ^
    -d "{\"prompt\": \"Explain the concept of machine learning in simple terms.\", \"model\": \"llama3.1:8b\", \"stream\": false, \"options\": {\"num_predict\": 100, \"temperature\": 0.7, \"top_p\": 0.9}}" ^
    "http://95.111.244.159:8000/api/chat"

echo.
echo ‚è±Ô∏è  Response Time: Measured
echo.

REM Test 3: Complex reasoning
echo üîç Test 3: Complex Reasoning Question
echo Prompt: "Compare cloud vs on-premises for business"
echo Max Tokens: 150
echo.

curl -s -w "HTTP: %%{http_code}, Time: %%{time_total}s" --max-time 120 ^
    -H "Content-Type: application/json" ^
    -H "Authorization: Basic bXljYTp3ajJZeXhqNGNxY1hnQ0E=" ^
    -d "{\"prompt\": \"Compare and contrast the benefits and drawbacks of using cloud computing versus on-premises infrastructure for a medium-sized business.\", \"model\": \"llama3.1:8b\", \"stream\": false, \"options\": {\"num_predict\": 150, \"temperature\": 0.7, \"top_p\": 0.9}}" ^
    "http://95.111.244.159:8000/api/chat"

echo.
echo ‚è±Ô∏è  Response Time: Measured
echo.

echo üìä Performance Analysis
echo ================================================
echo üéØ Expected Performance for Your Hardware (8 CPU, 24GB RAM):
echo   ‚Ä¢ Short questions (1-2 sentences): 2-8 seconds
echo   ‚Ä¢ Medium questions (3-5 sentences): 8-20 seconds
echo   ‚Ä¢ Long questions (paragraphs): 20-45 seconds
echo   ‚Ä¢ Complex reasoning tasks: 30-90 seconds
echo   ‚Ä¢ Expected throughput: 15-30 tokens/second
echo.

echo üîç Common Issues & Solutions:
echo   ‚Ä¢ Slow responses: Check CPU usage and model loading
echo   ‚Ä¢ Memory issues: Ensure 8-12GB free for Llama 3.1 8B
echo   ‚Ä¢ Network latency: Test local vs remote response times
echo   ‚Ä¢ Model optimization: Consider using quantized models
echo.

echo üöÄ Optimization Recommendations:
echo   ‚Ä¢ Use quantized models (Q4_K_M, Q5_K_M) for faster inference
echo   ‚Ä¢ Adjust context window size based on your use case
echo   ‚Ä¢ Monitor system resources during inference
echo   ‚Ä¢ Consider model switching for different complexity tasks
echo.

echo ‚úÖ Performance diagnostics completed!
echo üí° Check the results above to identify performance bottlenecks.
echo.

echo üîß Next Steps:
echo   1. Check VPS resource usage (CPU, RAM, disk)
echo   2. Verify Ollama model is properly loaded
echo   3. Consider switching to quantized model
echo   4. Monitor network latency between your location and VPS
echo.

pause
